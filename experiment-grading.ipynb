{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading Experiment\n",
    "MCQ grading and feedback\n",
    "\n",
    "Scenarios:\n",
    "1. Single agent 0-shot\n",
    "2. Single agent 1-shot\n",
    "3. 2 agents 0-shot\n",
    "4. 2 Agents 1-Shot (Grade -> Structured Feedback - Simulated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-genai in /Users/vh19/.pyenv/versions/3.12.6/lib/python3.12/site-packages (1.9.0)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /Users/vh19/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from google-genai) (4.9.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /Users/vh19/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from google-genai) (2.38.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /Users/vh19/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from google-genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /Users/vh19/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from google-genai) (2.11.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /Users/vh19/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from google-genai) (2.32.3)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /Users/vh19/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from google-genai) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /Users/vh19/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from google-genai) (4.13.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/vh19/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/vh19/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/vh19/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/vh19/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/vh19/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9)\n",
      "Requirement already satisfied: certifi in /Users/vh19/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/vh19/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/vh19/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/vh19/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in /Users/vh19/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/vh19/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vh19/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vh19/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /Users/vh19/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"gemini-2.0-flash\"\n",
    "api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "client = genai.Client(\n",
    "  api_key=api_key,\n",
    ")\n",
    "\n",
    "# setup study material files\n",
    "files = [\n",
    "  client.files.upload(file='section-loop.pdf'),\n",
    "  client.files.upload(file='sample-rubric.pdf'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_question_generator_config(system_prompt):\n",
    "  return types.GenerateContentConfig(\n",
    "      response_mime_type=\"application/json\",\n",
    "      system_instruction=[\n",
    "          types.Part.from_text(text=system_prompt),\n",
    "      ],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grading_result(questions, student_answers):\n",
    "    \"\"\"\n",
    "    Grade student answers against questions and return detailed results.\n",
    "    \n",
    "    Args:\n",
    "        questions (list): List of question dictionaries containing question text and correct answer\n",
    "        student_answers (list): List of student's answer choices\n",
    "        \n",
    "    Returns:\n",
    "        list: List of dictionaries containing grading results for each question\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for i, (question, student_answer) in enumerate(zip(questions, student_answers)):\n",
    "        result = {\n",
    "            'question_text': question['question_text'],\n",
    "            'student_answer': student_answer,\n",
    "            'correct_answer': question['correct_option'],\n",
    "            'is_correct': student_answer == question['correct_option']\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = json.load(open('sample-questions.json'))['questions']\n",
    "student_answers = json.load(open('sample-answer.json'))['answers']\n",
    "\n",
    "student_results = get_grading_result(questions, student_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_scenario_1 = \"\"\n",
    "with open('grader-0-shot-prompt.txt', 'r') as file:\n",
    "    system_prompt_scenario_1 = file.read()\n",
    "\n",
    "scenario_1_config = setup_question_generator_config(system_prompt_scenario_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_scenario_1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for student_result in student_results:\n",
    "    user_text = f\"\"\"Generate a justification/feedback message for the student's answer to the MCQ below. Follow the guidelines in the referenced rubric file and use the source text file for context when needed. Use the JSON output format described in your instructions.\n",
    "Question Text: \"{student_result['question_text']}\"\n",
    "Correct Answer Key: \"{student_result['correct_answer']}\"\n",
    "Student Selected Key: \"{student_result['student_answer']}\"\n",
    "Was Student Correct?: {student_result['is_correct']}\n",
    "(Source Text and Rubric files are referenced)\n",
    "\"\"\"\n",
    "    content_scenario_1 = [\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[\n",
    "                types.Part.from_uri(\n",
    "                    file_uri=files[0].uri,\n",
    "                    mime_type=files[0].mime_type,\n",
    "                ),\n",
    "                types.Part.from_uri(\n",
    "                    file_uri=files[1].uri,\n",
    "                    mime_type=files[1].mime_type,\n",
    "                ),\n",
    "                types.Part.from_text(text=user_text),\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    _result_1 = client.models.generate_content(\n",
    "        model=base_model,\n",
    "        contents=content_scenario_1,\n",
    "        config=scenario_1_config,\n",
    "    )\n",
    "    result_scenario_1.append(json.loads(_result_1.text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'justification_message': 'Correct! The while loop continues as long as the condition is true.'}\n",
      "{'justification_message': 'Correct! The continue statement skips the rest of the current iteration and proceeds to the next one.'}\n",
      "{'justification_message': 'The outer loop controls how many times the inner loop runs completely. Review section 5.3, which defines the roles of the outer and inner loops in nested structures.'}\n",
      "{'justification_message': \"The correct answer is C. The else block in a loop-else construct is executed when the loop completes its iterations without encountering a 'break' statement. If a 'break' statement is encountered during the loop's execution, the else block is skipped (page 137).\"}\n",
      "{'justification_message': 'Correct! The \"else\" block associated with the \"while\" loop is executed only when the loop finishes normally (i.e., the condition becomes false). In this case, the loop finishes when \\'i\\' is no longer less than 5, and \"Loop finished!\" is printed.'}\n"
     ]
    }
   ],
   "source": [
    "for result in result_scenario_1:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_scenario_2 = \"\"\n",
    "with open('grader-1-shot-prompt.txt', 'r') as file:\n",
    "    system_prompt_scenario_2 = file.read()\n",
    "\n",
    "scenario_2_config = setup_question_generator_config(system_prompt_scenario_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_scenario_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for student_result in student_results:\n",
    "    user_text = f\"\"\"Here is an example of the desired JSON output format and justification style (assuming it follows the rubric):\n",
    "Example Output (for an incorrect answer):\n",
    "{{\n",
    "  \"justification_message\": \"Not quite. The correct answer was C. The `break` statement exits the loop immediately. Your answer B describes the `continue` statement, which only skips the current iteration (see guideline 3 in the rubric file and the source text example on loop control).\"\n",
    "}}\n",
    "\n",
    "Now, using that exact JSON format and justification style, generate a justification/feedback message for the student's answer to the MCQ below. Follow the guidelines in the referenced rubric file and use the source text file for context when needed.\n",
    "\n",
    "Question Text: \"{student_result['question_text']}\"\n",
    "Correct Answer Key: \"{student_result['correct_answer']}\"\n",
    "Student Selected Key: \"{student_result['student_answer']}\"\n",
    "Was Student Correct?: \"{student_result['is_correct']}\"\n",
    "(Source Text and Rubric files are referenced)\n",
    "\"\"\"\n",
    "    content_scenario_2 = [\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[\n",
    "                types.Part.from_uri(\n",
    "                    file_uri=files[0].uri,\n",
    "                    mime_type=files[0].mime_type,\n",
    "                ),\n",
    "                types.Part.from_uri(\n",
    "                    file_uri=files[1].uri,\n",
    "                    mime_type=files[1].mime_type,\n",
    "                ),\n",
    "                types.Part.from_text(text=user_text),\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    _result_2 = client.models.generate_content(\n",
    "        model=base_model,\n",
    "        contents=content_scenario_2,\n",
    "        config=scenario_2_config,\n",
    "    )\n",
    "    result_scenario_2.append(json.loads(_result_2.text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'justification_message': 'Correct! The while loop continues as long as the condition is true.'}\n",
      "{'justification_message': 'Correct! The `continue` statement skips the rest of the current iteration and proceeds to the next iteration of the loop.'}\n",
      "{'justification_message': \"Incorrect. The outer loop determines how many times the inner loop runs in its entirety. Review the 'Nested loops' section.\"}\n",
      "{'justification_message': 'The correct answer is C. The `else` block in a loop `else` statement is executed only if the loop completes its iterations without encountering a `break` statement. Option B is incorrect; the `else` block *is* executed when the loop condition becomes false (naturally terminates). See section 5.5 in the text.'}\n",
      "{'justification_message': 'Correct! The `else` block associated with the `while` loop is executed after the loop finishes normally (i.e., when the condition `i < 5` is no longer true).'}\n"
     ]
    }
   ],
   "source": [
    "for result in result_scenario_2:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_reviewer_scenario_3 = \"\"\n",
    "with open('grading-reviewer-0-shot-prompt.txt', 'r') as file:\n",
    "    system_prompt_reviewer_scenario_3 = file.read()\n",
    "\n",
    "scenario_3_config = setup_question_generator_config(system_prompt_reviewer_scenario_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_result_scenario_3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> {\n",
      "  \"adheres_to_rubric\": false,\n",
      "  \"assessment_notes\": \"The justification is too short and doesn't properly explain the answer. The questions require explaining reasoning behind the selection and the generated answer does not attempt to do that.\"\n",
      "}\n",
      "<class 'str'> {\n",
      "  \"adheres_to_rubric\": true,\n",
      "  \"assessment_notes\": \"The justification message is very brief but accurate in this case. It correctly identifies what the \\\"continue\\\" statement does. It's concise and doesn't violate any of the negative constraints.\"\n",
      "}\n",
      "<class 'str'> {\n",
      "  \"adheres_to_rubric\": true,\n",
      "  \"assessment_notes\": \"The justification accurately and concisely refers to the location in the text where the answer can be found (section 5.3).\"\n",
      "}\n",
      "<class 'str'> {\n",
      "  \"adheres_to_rubric\": true,\n",
      "  \"assessment_notes\": \"The justification accurately and concisely explains the concept of a 'loop-else' construct. It correctly identifies that the 'else' block is executed only when the loop completes without a 'break' and correctly indicates the page where this is discussed. The justification is clear, and directly addresses the functionality in question.\"\n",
      "}\n",
      "<class 'str'> {\n",
      "  \"adheres_to_rubric\": true,\n",
      "  \"assessment_notes\": \"The justification is concise and accurately explains the behavior of the `else` block in relation to the `while` loop. It clearly states the condition for the `else` block's execution. The response adheres to the length constraints and maintains an appropriate tone.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for result in result_scenario_1:\n",
    "    user_text = f\"\"\"Assess whether the following justification message adheres to all guidelines specified in the referenced rubric file. Use the JSON output format described in your instructions.\n",
    "Justification to Review:\n",
    "\n",
    "{result}\n",
    "\n",
    "(Rubric file and study material are attached)\n",
    "\"\"\"\n",
    "\n",
    "    content_scenario_3 = [\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[\n",
    "                types.Part.from_uri(\n",
    "                    file_uri=files[0].uri,\n",
    "                    mime_type=files[0].mime_type,\n",
    "                ),\n",
    "                types.Part.from_uri(\n",
    "                    file_uri=files[1].uri,\n",
    "                    mime_type=files[1].mime_type,\n",
    "                ),\n",
    "                types.Part.from_text(text=user_text),\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    _result_3 = client.models.generate_content(\n",
    "        model=base_model,\n",
    "        contents=content_scenario_3,\n",
    "        config=scenario_3_config,\n",
    "    )\n",
    "    print(f\"{type(_result_3.text)} {_result_3.text}\")\n",
    "    feedback_result_scenario_3.append(json.loads(_result_3.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adheres_to_rubric': False, 'assessment_notes': \"The justification is too short and doesn't properly explain the answer. The questions require explaining reasoning behind the selection and the generated answer does not attempt to do that.\"}\n",
      "{'adheres_to_rubric': True, 'assessment_notes': 'The justification message is very brief but accurate in this case. It correctly identifies what the \"continue\" statement does. It\\'s concise and doesn\\'t violate any of the negative constraints.'}\n",
      "{'adheres_to_rubric': True, 'assessment_notes': 'The justification accurately and concisely refers to the location in the text where the answer can be found (section 5.3).'}\n",
      "{'adheres_to_rubric': True, 'assessment_notes': \"The justification accurately and concisely explains the concept of a 'loop-else' construct. It correctly identifies that the 'else' block is executed only when the loop completes without a 'break' and correctly indicates the page where this is discussed. The justification is clear, and directly addresses the functionality in question.\"}\n",
      "{'adheres_to_rubric': True, 'assessment_notes': \"The justification is concise and accurately explains the behavior of the `else` block in relation to the `while` loop. It clearly states the condition for the `else` block's execution. The response adheres to the length constraints and maintains an appropriate tone.\"}\n"
     ]
    }
   ],
   "source": [
    "for result in feedback_result_scenario_3:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_result_scenario_3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> {\n",
      "\"justification_message\": \"Correct! The while loop continues as long as the condition is true.\"\n",
      "}\n",
      "<class 'str'> {\n",
      "  \"justification_message\": \"Correct! The continue statement skips the rest of the current iteration and proceeds to the next one.\"\n",
      "}\n",
      "<class 'str'> {\n",
      "\"justification_message\": \"Incorrect. The outer loop controls the number of times the inner loop executes fully. See section 5.3.\"\n",
      "}\n",
      "<class 'str'> {\n",
      " \"justification_message\": \"That's incorrect. According to section 5.5, the `else` block of a loop is executed only when the loop completes its iterations without encountering a `break` statement. If a `break` is encountered, the `else` block is skipped.\"\n",
      "}\n",
      "<class 'str'> {\n",
      "\"justification_message\": \"Correct! The 'else' block associated with a 'while' loop is executed only if the loop completes normally (i.e., the loop condition becomes false), and not if the loop is terminated by a 'break' statement.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for i, student_result in enumerate(student_results):\n",
    "    user_text = f\"\"\"Generate a justification/feedback message for the student's answer to the MCQ below. Follow the guidelines in the referenced rubric file and use the source text file for context when needed. Use the JSON output format described in your instructions.\n",
    "Question Text: \"{student_result['question_text']}\"\n",
    "Correct Answer Key: \"{student_result['correct_answer']}\"\n",
    "Student Selected Key: \"{student_result['student_answer']}\"\n",
    "Was Student Correct?: {student_result['is_correct']}\n",
    "(Source Text and Rubric files are referenced)\n",
    "\"\"\"\n",
    "    content_scenario_3 = [\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[\n",
    "                types.Part.from_uri(\n",
    "                    file_uri=files[0].uri,\n",
    "                    mime_type=files[0].mime_type,\n",
    "                ),\n",
    "                types.Part.from_uri(\n",
    "                    file_uri=files[1].uri,\n",
    "                    mime_type=files[1].mime_type,\n",
    "                ),\n",
    "                types.Part.from_text(text=user_text),\n",
    "                types.Part.from_text(text=f\"\"\"Feedback from review agent:{feedback_result_scenario_3[i]}\"\"\"),\n",
    "            ]\n",
    "        ),\n",
    "    ]\n",
    "    new_result_3 = client.models.generate_content(\n",
    "        model=base_model,\n",
    "        contents=content_scenario_3,\n",
    "        config=scenario_1_config,\n",
    "    )\n",
    "    print(f\"{type(new_result_3.text)} {new_result_3.text}\")\n",
    "    new_result_scenario_3.append(json.loads(new_result_3.text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'justification_message': 'Correct! The while loop continues as long as the condition is true.'}\n",
      "{'justification_message': 'Correct! The continue statement skips the rest of the current iteration and proceeds to the next one.'}\n",
      "{'justification_message': 'Incorrect. The outer loop controls the number of times the inner loop executes fully. See section 5.3.'}\n",
      "{'justification_message': \"That's incorrect. According to section 5.5, the `else` block of a loop is executed only when the loop completes its iterations without encountering a `break` statement. If a `break` is encountered, the `else` block is skipped.\"}\n",
      "{'justification_message': \"Correct! The 'else' block associated with a 'while' loop is executed only if the loop completes normally (i.e., the loop condition becomes false), and not if the loop is terminated by a 'break' statement.\"}\n"
     ]
    }
   ],
   "source": [
    "for result in new_result_scenario_3:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_reviewer_scenario_4 = \"\"\n",
    "with open('grading-reviewer-1-shot-prompt.txt', 'r') as file:\n",
    "    system_prompt_reviewer_scenario_4 = file.read()\n",
    "\n",
    "scenario_4_config = setup_question_generator_config(system_prompt_reviewer_scenario_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_result_scenario_4 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in result_scenario_2:\n",
    "    user_text = f\"\"\"Here is an example of the desired JSON output format for the assessment:\n",
    "Example Output (assuming adherence):\n",
    "{{\n",
    "  \"adheres_to_rubric\": true,\n",
    "  \"assessment_notes\": \"Adheres to all guidelines: Correct length, supportive tone, explains using source concepts, states correct answer.\"\n",
    "}}\n",
    "// OR (assuming non-adherence):\n",
    "// {{\n",
    "//  \"adheres_to_rubric\": false,\n",
    "//  \"assessment_notes\": \"Does not adhere: Failed to explain why the student's incorrect answer was wrong (guideline 2b) and used a neutral rather than supportive tone (guideline 1).\"\n",
    "// }}\n",
    "\n",
    "Now, using that exact JSON format, assess whether the following justification message adheres to all guidelines specified in the referenced rubric file.\n",
    "\n",
    "Justification to Review:\n",
    "{result}\n",
    "\n",
    "(Rubric file and study material are attached)\n",
    "\"\"\"\n",
    "\n",
    "    content_scenario_4 = [\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[\n",
    "                types.Part.from_uri(\n",
    "                    file_uri=files[0].uri,\n",
    "                    mime_type=files[0].mime_type,\n",
    "                ),\n",
    "                types.Part.from_uri(\n",
    "                    file_uri=files[1].uri,\n",
    "                    mime_type=files[1].mime_type,\n",
    "                ),\n",
    "                types.Part.from_text(text=user_text),\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    _result_4 = client.models.generate_content(\n",
    "        model=base_model,\n",
    "        contents=content_scenario_4,\n",
    "        config=scenario_4_config,\n",
    "    )\n",
    "    feedback_result_scenario_4.append(json.loads(_result_4.text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adheres_to_rubric': False, 'assessment_notes': 'The justification is too short and does not provide enough explanation. It also does not refer to the source text. It is also too terse to be supportive.'}\n",
      "{'adheres_to_rubric': True, 'assessment_notes': \"The justification message is concise, uses a supportive tone ('Correct!'), and accurately describes the functionality of the `continue` statement in the context of loops. It directly relates to the concept being tested in the material.\"}\n",
      "{'adheres_to_rubric': False, 'assessment_notes': \"The justification uses a neutral, rather than supportive tone. It also doesn't state the correct answer or give specific guidance on how to arrive at it, which is especially important given that the response was incorrect.\"}\n",
      "{'adheres_to_rubric': True, 'assessment_notes': 'Adheres to all guidelines. The justification is concise, uses a supportive tone, and correctly explains the logic behind the correct answer (C), while also explaining why option B is incorrect, based on the content in section 5.5.'}\n",
      "{'adheres_to_rubric': True, 'assessment_notes': 'The justification correctly identifies the behavior of the else statement in conjunction with the while loop. It uses a supportive tone and explains the correct answer.'}\n"
     ]
    }
   ],
   "source": [
    "for result in feedback_result_scenario_4:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_result_scenario_4 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "for student_result in student_results:\n",
    "    user_text = f\"\"\"Here is an example of the desired JSON output format and justification style (assuming it follows the rubric):\n",
    "Example Output (for an incorrect answer):\n",
    "{{\n",
    "  \"justification_message\": \"Not quite. The correct answer was C. The `break` statement exits the loop immediately. Your answer B describes the `continue` statement, which only skips the current iteration (see guideline 3 in the rubric file and the source text example on loop control).\"\n",
    "}}\n",
    "\n",
    "Now, using that exact JSON format and justification style, generate a justification/feedback message for the student's answer to the MCQ below. Follow the guidelines in the referenced rubric file and use the source text file for context when needed.\n",
    "\n",
    "Question Text: \"{student_result['question_text']}\"\n",
    "Correct Answer Key: \"{student_result['correct_answer']}\"\n",
    "Student Selected Key: \"{student_result['student_answer']}\"\n",
    "Was Student Correct?: \"{student_result['is_correct']}\"\n",
    "(Source Text and Rubric files are referenced)\n",
    "\"\"\"\n",
    "    content_scenario_2 = [\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[\n",
    "                types.Part.from_uri(\n",
    "                    file_uri=files[0].uri,\n",
    "                    mime_type=files[0].mime_type,\n",
    "                ),\n",
    "                types.Part.from_uri(\n",
    "                    file_uri=files[1].uri,\n",
    "                    mime_type=files[1].mime_type,\n",
    "                ),\n",
    "                types.Part.from_text(text=user_text),\n",
    "                types.Part.from_text(text=f\"\"\"Feedback from review agent: {feedback_result_scenario_4[i]}\"\"\"),\n",
    "            ]\n",
    "        ),\n",
    "    ]\n",
    "    new_result_4 = client.models.generate_content(\n",
    "        model=base_model,\n",
    "        contents=content_scenario_2,\n",
    "        config=scenario_2_config,\n",
    "    )\n",
    "    new_result_scenario_4.append(json.loads(new_result_4.text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'justification_message': 'Correct! A while loop continues executing as long as its condition remains true.'}\n",
      "{'justification_message': \"That's correct! The `continue` statement skips the rest of the current iteration and proceeds to the next (see section 5.4 in the text).\"}\n",
      "{'justification_message': 'Not quite! The outer loop determines how many times the inner loop runs completely. Therefore, the correct answer is B (outer loop). Review section 5.3 in the text for more information on nested loops.'}\n",
      "{'justification_message': \"Incorrect. The correct answer is C: If the loop's execution is interrupted by a `break` statement. According to the text, a loop `else` statement runs after the loop's execution is completed without being interrupted by a `break` statement.\"}\n",
      "{'justification_message': 'Correct! The `else` block associated with a `while` loop is executed only when the loop finishes normally (i.e., the loop condition becomes false), not if the loop is terminated by a `break` statement.'}\n"
     ]
    }
   ],
   "source": [
    "for result in new_result_scenario_4:\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
